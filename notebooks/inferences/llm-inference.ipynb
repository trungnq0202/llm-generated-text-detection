{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/kaggle/input/omegaconf')\n",
    "sys.path.insert(0, '/kaggle/input/utils-ai-v3')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from omegaconf import OmegaConf\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "sys.path.append(\"../first_place_reproduction\")\n",
    "from dataset import AiDataset\n",
    "from dataloader import AiCollator, show_batch\n",
    "from model import MistralForDetectAI\n",
    "\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process -----\n",
    "char_to_remove = ['{', '£', '\\x97', '¹', 'å', '\\\\', '\\x85', '<', '\\x99', \\\n",
    "                  'é', ']', '+', 'Ö', '\\xa0', '>', '|', '\\x80', '~', '©', \\\n",
    "                  '/', '\\x93', '$', 'Ó', '²', '^', ';', '`', 'á', '*', '(', \\\n",
    "                  '¶', '®', '[', '\\x94', '\\x91', '#', '-', 'ó', ')', '}', '=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, strategy='light'):\n",
    "    assert strategy in [\"none\", \"light\", \"heavy\"], \"pre-processing strategy must one of: none, light, heavy\"\n",
    "    \n",
    "    if strategy == \"none\":\n",
    "        text = text\n",
    "        \n",
    "    elif strategy == \"light\":\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode('ascii')        \n",
    "        text = text.strip()\n",
    "        text = text.strip(\"\\\"\")\n",
    "\n",
    "        for c in char_to_remove:\n",
    "            text = text.replace(c, \"\")\n",
    "\n",
    "        if text[-1]!=\".\":\n",
    "            text = text.split(\".\")\n",
    "            text = \".\".join(text[:-1])\n",
    "            text += \".\"\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s.,;?!:()\\'\\\"%-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(accelerator, model, infer_dl, example_ids):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "\n",
    "    progress_bar = tqdm(range(len(infer_dl)), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    for step, batch in enumerate(infer_dl):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits.reshape(-1)\n",
    "        predictions = torch.sigmoid(logits)\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        predictions = predictions.cpu().numpy().tolist()\n",
    "\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"id\"] = example_ids\n",
    "    result_df[\"generated\"] = all_predictions\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cfg, save_dir, model_id):\n",
    "    \n",
    "    # create accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # read test data\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        test_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\", sep=',')\n",
    "    else:\n",
    "        test_df = pd.read_csv(\"/kaggle/input/mock-test/test.csv\", sep=',')\n",
    "        \n",
    "    accelerator.print(\"~~\"*40)\n",
    "    accelerator.print(f\"PRE-PROCESSING: {cfg.preprocess_strategy.upper()}\")\n",
    "    accelerator.print(\"~~\"*40)\n",
    "\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: preprocess_text(x, cfg.preprocess_strategy))\n",
    "    accelerator.print(f'Test csv shape: {test_df.shape}')\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        dataset_creator = AiDataset(cfg)\n",
    "        infer_ds = dataset_creator.get_dataset(test_df)\n",
    "    \n",
    "    tokenizer = dataset_creator.tokenizer\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    infer_ds = infer_ds.sort(\"input_length\")\n",
    "    infer_ds.set_format(\n",
    "        type=None,\n",
    "        columns=[\n",
    "            'id',\n",
    "            'input_ids',\n",
    "            'attention_mask',\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    infer_ids = infer_ds[\"id\"]  # .tolist()\n",
    "    \n",
    "    #--\n",
    "    data_collator = AiCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        pad_to_multiple_of=64\n",
    "    )\n",
    "\n",
    "    infer_dl = DataLoader(\n",
    "        infer_ds,\n",
    "        batch_size=cfg.predict_params.per_device_eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    accelerator.print(\"data preparation done...\")\n",
    "    accelerator.print(\"~~\"*40)\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    \n",
    "    #----------\n",
    "    for b in infer_dl:\n",
    "        break\n",
    "    show_batch(b, tokenizer, task='infer', print_fn=accelerator.print)\n",
    "    accelerator.print(\"~~\"*40)\n",
    "    #----------\n",
    "\n",
    "\n",
    "    ## Load Model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    base_model = MistralForDetectAI.from_pretrained(\n",
    "        cfg.model.backbone_path,\n",
    "        num_labels=cfg.model.num_labels,\n",
    "        quantization_config=bnb_config,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    base_model.config.pretraining_tp = 1\n",
    "    # base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = PeftModel.from_pretrained(base_model, cfg.model.lora_path)\n",
    "    accelerator.print(\"### Loaded Model Weights ###\")\n",
    "    \n",
    "    model, infer_dl = accelerator.prepare(model, infer_dl)\n",
    "    \n",
    "    # run inference ---\n",
    "    sub_df = run_inference(accelerator, model, infer_dl, infer_ids)\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        save_path = os.path.join(save_dir, f\"{model_id}.parquet\")\n",
    "        sub_df.to_parquet(save_path)\n",
    "        accelerator.print(\"done!\")\n",
    "        accelerator.print(\"~~\"*40)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

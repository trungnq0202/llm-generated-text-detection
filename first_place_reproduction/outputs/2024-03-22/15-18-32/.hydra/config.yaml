seed: 424
use_wandb: false
input_data_dir: datasets/external/ai_mix_v16
model:
  backbone_path: mistralai/Mistral-7B-v0.1
  max_length: 1296
  num_labels: 1
  tokenizer:
    padding_side: left
    truncation_side: left
    use_fast: true
  lora:
    target_modules:
    - q_proj
    - k_proj
    r: 8
    lora_alpha: 16
    lora_dropout: 0.1
    modules_to_save:
    - classification_head
train_params:
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  num_train_epochs: 1
  gradient_accumulation_steps: 4
outputs:
  model_dir: models/r_detect_mix_v16
